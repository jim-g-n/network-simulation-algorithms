{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import ndlib.models.epidemics as ep\n",
    "import ndlib.models.ModelConfig as mc\n",
    "from ndlib.utils import multi_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms used in aggregation simulation on synthetic networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_nodes(G, node_0, node_1): # aggregating two nodes in graph G\n",
    "\n",
    "    edges_0 = G.edges(node_0, data=True)\n",
    "    edges_1 = G.edges(node_1, data=True) # edges for the two nodes\n",
    "    \n",
    "    new_edges = []\n",
    "\n",
    "    for i, (s, t, data) in enumerate(edges_1): # taking edges from node 1 and moving them to node 0        \n",
    "        if (s!=t):\n",
    "            new_edges.append((node_0, t, data))\n",
    "    \n",
    "    G.remove_node(node_1)\n",
    "    G.add_edges_from(new_edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def shared_neighbours(G, node_0, node_1, min_share = True): # calculating proportion of shared neighbours\n",
    "    neighbours_0 = [n for n in G.neighbors(node_0)]\n",
    "    neighbours_0 = [x for x in neighbours_0 if x != node_1]\n",
    "    neighbours_1 = [n for n in G.neighbors(node_1)]\n",
    "    neighbours_1 = [x for x in neighbours_1 if x != node_0]\n",
    "    shared_nodes = [x for x in neighbours_0 if x in neighbours_1]\n",
    "    if (len(neighbours_0)*len(neighbours_1)!=0):\n",
    "        if min_share: # whether to use smaller or larger proportion shared neighbours\n",
    "            return min(len(shared_nodes)/len(neighbours_0), len(shared_nodes)/len(neighbours_1))\n",
    "        else:\n",
    "            return max(len(shared_nodes)/len(neighbours_0), len(shared_nodes)/len(neighbours_1))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node aggregation, SIR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of population infected for different aggregation\n",
    "rand_agg = []\n",
    "non_rand_agg = []\n",
    "full_res = []\n",
    "\n",
    "nodes_aggregated = []\n",
    "graph_num = []\n",
    "\n",
    "#average degrees\n",
    "non_rand_ave_deg = []\n",
    "rand_ave_deg = []\n",
    "\n",
    "# peak infected numbers\n",
    "peak_inf_full = []\n",
    "peak_inf_non_rand = []\n",
    "peak_inf_rand = []\n",
    "\n",
    "# times to stability\n",
    "stability_full = []\n",
    "stability_non_rand = []\n",
    "stability_rand = []\n",
    "\n",
    "# times to peak infection\n",
    "peak_time_full = []\n",
    "peak_time_non_rand = []\n",
    "peak_time_rand = []\n",
    "\n",
    "config = mc.Configuration()\n",
    "config.add_model_parameter('beta', 0.001) # model parameters\n",
    "config.add_model_parameter('gamma', 0.2)\n",
    "config.add_model_parameter(\"fraction_infected\", 0.05)\n",
    "\n",
    "num_nodes = 2500\n",
    "a = 4 # parameter for skewed distribution values\n",
    "\n",
    "for num_graphs in range(1): # independent graphs to test\n",
    "    graph_num.append(num_graphs)\n",
    "    G = nx.watts_strogatz_graph(2500, 250, 0.1) # generating the graph\n",
    "    \n",
    "    keys = np.arange(num_nodes) # drawing from the distribution and binning\n",
    "    values = skewnorm.rvs(a, size=num_nodes)\n",
    "    num_bins = 50 \n",
    "    \n",
    "    digitized = np.digitize(values, np.linspace(min(values), max(values), num=num_bins)) # labelling nodes by their bins\n",
    "\n",
    "    node_mapping = dict(zip(G.nodes(), sorted(G.nodes(), key=lambda k: random.random()))) # randomly relabelling nodes\n",
    "    G = nx.relabel_nodes(G, node_mapping)\n",
    "    \n",
    "    model = ep.SIRModel(G) # configuring model\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    trends = multi_runs(model, execution_number=10, iteration_number=2500, infection_sets=None, nprocesses=5) # running model on initial graph\n",
    "\n",
    "    # calculating results of model on unaggregated graph\n",
    "    full_res.append([(len(list(G.nodes())) - x)/len(list(G.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]])\n",
    "    stability_full.append([trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)])\n",
    "    peak_inf_full.append([max(x)/len(list(G.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]])\n",
    "    peak_time_full.append([trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    g_cop = G.copy() # copy of graph to aggregate\n",
    "    join_perc = 0.8 # shared neighbour threshold parameter\n",
    "    for i in range(1,num_bins+1): # doing pairwise comparisons bin by bin\n",
    "        relevant_nodes = keys[digitized==i]\n",
    "        for j in relevant_nodes:\n",
    "            if (j in g_cop.nodes()): \n",
    "                for k in relevant_nodes:\n",
    "                    if (k in g_cop.nodes()) and (k != j): # if nodes haven't been removed by aggregation\n",
    "                        if shared_neighbours(g_cop, j, k, min_share=False) > join_perc: # use larger proportion, join if they exceed\n",
    "                            g_cop = join_nodes(g_cop, j, k)\n",
    "\n",
    "    model = ep.SIRModel(g_cop)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    trends = multi_runs(model, execution_number=10, iteration_number=2500, infection_sets=None, nprocesses=5) # running model on shared neighbour aggregated graph\n",
    "\n",
    "    non_rand_agg.append([(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]])\n",
    "    stability_non_rand.append([trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)])\n",
    "    peak_inf_non_rand.append([max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]])\n",
    "    peak_time_non_rand.append([trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)])\n",
    "    \n",
    "    \n",
    "    non_rand_ave_deg.append(sum(nx.degree_centrality(g_cop).values())/len(g_cop.nodes()))\n",
    "    \n",
    "    combinations = num_nodes - len(g_cop.nodes()) # number of aggregations that took place\n",
    "\n",
    "    nodes_aggregated.append(combinations)\n",
    "\n",
    "    g_cop = G.copy() # another copy of graph to do random aggregation\n",
    "    for j in range(combinations): # joining nodes at random\n",
    "        all_nodes = list(g_cop.nodes())\n",
    "        choices = np.random.choice(range(len(all_nodes)), 2, replace=False)\n",
    "        g_cop = join_nodes(g_cop, all_nodes[choices[0]], all_nodes[choices[1]])\n",
    "\n",
    "    model = ep.SIRModel(g_cop)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    trends = multi_runs(model, execution_number=10, iteration_number=2500, infection_sets=None, nprocesses=5) # running model on randomly aggregated graph\n",
    "\n",
    "    rand_agg.append([(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]])\n",
    "    stability_rand.append([trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)])\n",
    "    peak_inf_rand.append([max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]])\n",
    "    peak_time_rand.append([trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)])\n",
    "    \n",
    "    \n",
    "    rand_ave_deg.append(sum(nx.degree_centrality(g_cop).values())/len(g_cop.nodes()))\n",
    "    \n",
    "aggregation_results = pd.DataFrame({'graph_num':graph_num, 'nodes_aggregated': nodes_aggregated, 'full_res': full_res, 'non_rand_agg': non_rand_agg, 'rand_agg': rand_agg, 'non_rand_ave_deg': non_rand_ave_deg, 'rand_ave_deg': rand_ave_deg,\n",
    "                                   'peak_inf_full': peak_inf_full, 'peak_inf_non_rand': peak_inf_non_rand, 'peak_inf_rand': peak_inf_rand, 'stability_full': stability_full,\n",
    "                                   'stability_non_rand': stability_non_rand, 'stability_rand': stability_rand, 'peak_time_full': peak_time_full, 'peak_time_non_rand': peak_time_non_rand, 'peak_time_rand': peak_time_rand})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graph_num</th>\n",
       "      <th>nodes_aggregated</th>\n",
       "      <th>full_res</th>\n",
       "      <th>non_rand_agg</th>\n",
       "      <th>rand_agg</th>\n",
       "      <th>non_rand_ave_deg</th>\n",
       "      <th>rand_ave_deg</th>\n",
       "      <th>peak_inf_full</th>\n",
       "      <th>peak_inf_non_rand</th>\n",
       "      <th>peak_inf_rand</th>\n",
       "      <th>stability_full</th>\n",
       "      <th>stability_non_rand</th>\n",
       "      <th>stability_rand</th>\n",
       "      <th>peak_time_full</th>\n",
       "      <th>peak_time_non_rand</th>\n",
       "      <th>peak_time_rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>841</td>\n",
       "      <td>[0.414, 0.4484, 0.416, 0.5028, 0.4816, 0.5508,...</td>\n",
       "      <td>[0.21760096443640747, 0.35443037974683544, 0.3...</td>\n",
       "      <td>[0.6305003013863774, 0.6582278481012658, 0.719...</td>\n",
       "      <td>0.117581</td>\n",
       "      <td>0.200351</td>\n",
       "      <td>[0.0708, 0.0704, 0.0628, 0.0672, 0.07, 0.082, ...</td>\n",
       "      <td>[0.050030138637733576, 0.060880048221820374, 0...</td>\n",
       "      <td>[0.13743218806509946, 0.14647377938517178, 0.1...</td>\n",
       "      <td>[72, 86, 67, 90, 87, 73, 74, 87, 92, 98]</td>\n",
       "      <td>[51, 64, 84, 73, 69, 62, 60, 68, 53, 90]</td>\n",
       "      <td>[50, 41, 58, 68, 58, 47, 49, 59, 49, 61]</td>\n",
       "      <td>[10, 15, 15, 17, 12, 20, 5, 24, 10, 14]</td>\n",
       "      <td>[1, 8, 2, 10, 0, 6, 0, 6, 0, 0]</td>\n",
       "      <td>[12, 14, 14, 14, 15, 16, 15, 16, 15, 11]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   graph_num  nodes_aggregated  \\\n",
       "0          0               841   \n",
       "\n",
       "                                            full_res  \\\n",
       "0  [0.414, 0.4484, 0.416, 0.5028, 0.4816, 0.5508,...   \n",
       "\n",
       "                                        non_rand_agg  \\\n",
       "0  [0.21760096443640747, 0.35443037974683544, 0.3...   \n",
       "\n",
       "                                            rand_agg  non_rand_ave_deg  \\\n",
       "0  [0.6305003013863774, 0.6582278481012658, 0.719...          0.117581   \n",
       "\n",
       "   rand_ave_deg                                      peak_inf_full  \\\n",
       "0      0.200351  [0.0708, 0.0704, 0.0628, 0.0672, 0.07, 0.082, ...   \n",
       "\n",
       "                                   peak_inf_non_rand  \\\n",
       "0  [0.050030138637733576, 0.060880048221820374, 0...   \n",
       "\n",
       "                                       peak_inf_rand  \\\n",
       "0  [0.13743218806509946, 0.14647377938517178, 0.1...   \n",
       "\n",
       "                             stability_full  \\\n",
       "0  [72, 86, 67, 90, 87, 73, 74, 87, 92, 98]   \n",
       "\n",
       "                         stability_non_rand  \\\n",
       "0  [51, 64, 84, 73, 69, 62, 60, 68, 53, 90]   \n",
       "\n",
       "                             stability_rand  \\\n",
       "0  [50, 41, 58, 68, 58, 47, 49, 59, 49, 61]   \n",
       "\n",
       "                            peak_time_full               peak_time_non_rand  \\\n",
       "0  [10, 15, 15, 17, 12, 20, 5, 24, 10, 14]  [1, 8, 2, 10, 0, 6, 0, 6, 0, 0]   \n",
       "\n",
       "                             peak_time_rand  \n",
       "0  [12, 14, 14, 14, 15, 16, 15, 16, 15, 11]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Edge Removal, Threshold Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.DataFrame(columns=['average_degree', 'num_infected', 'ave_num_infected', 'graph_num'])\n",
    "\n",
    "for graph_num in range(1): # independent graphs to test\n",
    "\n",
    "    g_cop = nx.gnm_random_graph(2500, 312500) # Creating the graph\n",
    "\n",
    "    total_edges = len(list(g_cop.edges()))\n",
    "    edges_to_remove = total_edges-1250\n",
    "    remove_per_it = np.floor(edges_to_remove/50) # Number of edges to remove in 50 batches to have 1250 edges remaining (ave deg 1)\n",
    "\n",
    "    config = mc.Configuration() # Setting model parameters\n",
    "    frac_inf = 0.36\n",
    "    config.add_model_parameter('fraction_infected', frac_inf)\n",
    "    for i in g_cop.nodes():\n",
    "            config.add_node_configuration(\"threshold\", i, 0.50)\n",
    "\n",
    "    average_degree = [sum(nx.degree_centrality(g_cop).values())/2500]\n",
    "\n",
    "    model = ep.ThresholdModel(g_cop)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    trends = multi_runs(model, execution_number=10, iteration_number=30, infection_sets=None, nprocesses=5) # running the model\n",
    "\n",
    "    num_infected = [[trends[j]['trends']['node_count'][1][-1] for j in range(10)]]\n",
    "\n",
    "    for i in range(50): # removing the edges in batches and running the model at each step\n",
    "        all_edges = list(g_cop.edges())\n",
    "        choices = np.random.choice(range(len(all_edges)), int(remove_per_it), replace=False)\n",
    "        for choice in choices: # randomly removing edges\n",
    "            g_cop.remove_edge(all_edges[choice][0], all_edges[choice][1])\n",
    "        average_degree.append(sum(nx.degree_centrality(g_cop).values())/2500)\n",
    "\n",
    "        model = ep.ThresholdModel(g_cop)\n",
    "        model.set_initial_status(config)\n",
    "\n",
    "        trends = multi_runs(model, execution_number=10, iteration_number=30, infection_sets=None, nprocesses=5) # running the model on the graph with edges removed\n",
    "\n",
    "        num_infected.append([trends[j]['trends']['node_count'][1][-1] for j in range(10)])\n",
    "\n",
    "    df = pd.DataFrame({'average_degree': average_degree, 'num_infected':num_infected})\n",
    "    df['ave_num_infected'] = df['num_infected'].apply(lambda col: np.array(col).mean())\n",
    "    df['graph_num'] = graph_num\n",
    "    \n",
    "    all_results_df = pd.concat([all_results_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Node Removal, SIR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.DataFrame(columns=['num_infected', 'ave_num_infected', 'stability_time', 'ave_stability_time','peak_infected','peak_infected_time','ave_peak_infected','ave_peak_infected_time','graph_num'])\n",
    "\n",
    "for set_num in range(1): # independent graphs to test\n",
    "    for graph_num in range(1):\n",
    "        g_cop = nx.barabasi_albert_graph(2500,132) # creating graphs\n",
    "\n",
    "        all_nodes = list(g_cop.nodes())\n",
    "\n",
    "        choices = np.random.choice(range(len(all_nodes)),1250, replace=False) # randomly removing half of the nodes first since the model always fully infects above this point\n",
    "        g_cop.remove_nodes_from(choices)\n",
    "\n",
    "        total_nodes = len(list(g_cop.nodes()))\n",
    "\n",
    "        remove_per_it = np.floor((total_nodes-0.05*2500)/35) # number of nodes to remove in 35 batches to have 5% remaining at the end\n",
    "\n",
    "        config = mc.Configuration() # model parameters\n",
    "        config.add_model_parameter('gamma', 0.01)\n",
    "        config.add_model_parameter(\"fraction_infected\", 0.05)\n",
    "        config.add_model_parameter('beta', 0.001)\n",
    "\n",
    "        model = ep.SIRModel(g_cop)\n",
    "        model.set_initial_status(config)\n",
    "\n",
    "        trends = multi_runs(model, execution_number=10, iteration_number=1500, infection_sets=None, nprocesses=5) # running the model\n",
    "\n",
    "        num_infected = [[(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]]]\n",
    "        stability_time = [[trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)]]\n",
    "\n",
    "        peak_infected =[[max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]]]\n",
    "        peak_infected_time = [[trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)]]\n",
    "\n",
    "        nodes = [len(list(g_cop.nodes()))]\n",
    "        for i in range(35): # removing nodes in 35 batches\n",
    "            all_nodes = list(g_cop.nodes())\n",
    "            choices = np.random.choice(all_nodes, int(remove_per_it), replace=False)\n",
    "            g_cop.remove_nodes_from(choices) # removing nodes at random\n",
    "\n",
    "            model = ep.SIRModel(g_cop)\n",
    "            model.set_initial_status(config)\n",
    "\n",
    "            trends = multi_runs(model, execution_number=10, iteration_number=1500, infection_sets=None, nprocesses=5) # running the model after removing the nodes\n",
    "\n",
    "            num_infected.append([(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]])\n",
    "            stability_time.append([trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)])\n",
    "\n",
    "            peak_infected.append([max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]])\n",
    "            peak_infected_time.append([trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)])\n",
    "\n",
    "            nodes.append(len(list(g_cop.nodes())))\n",
    "        df = pd.DataFrame({'nodes': nodes, 'num_infected':num_infected, 'stability_time':stability_time, 'peak_infected': peak_infected, 'peak_infected_time': peak_infected_time})\n",
    "        df['ave_num_infected'] = df['num_infected'].apply(lambda col: np.array(col).mean())\n",
    "        df['ave_stability_time'] = df['stability_time'].apply(lambda col: np.array(col).mean())\n",
    "        df['ave_peak_infected'] = df['peak_infected'].apply(lambda col: np.array(col).mean())\n",
    "        df['ave_peak_infected_time'] = df['peak_infected_time'].apply(lambda col: np.array(col).mean())\n",
    "\n",
    "        df['graph_num'] = graph_num + set_num*4\n",
    "\n",
    "        all_results_df = pd.concat([all_results_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Node Removal, Arxiv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_G = nx.read_edgelist(\"cit-HepTh.txt.gz\")\n",
    "arxiv_G = arxiv_G.to_undirected()\n",
    "arxiv_G = nx.convert_node_labels_to_integers(arxiv_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000913735428202936"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*len(arxiv_G.edges())/len(list(arxiv_G.nodes()))/len(list(arxiv_G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://snap.stanford.edu/data/cit-HepTh.html\n",
    "\n",
    "arxiv_G = nx.read_edgelist(\"cit-HepTh.txt.gz\")\n",
    "arxiv_G = arxiv_G.to_undirected()\n",
    "arxiv_G = nx.convert_node_labels_to_integers(arxiv_G)\n",
    "\n",
    "all_results_df = pd.DataFrame(columns=['num_infected', 'ave_num_infected', 'stability_time', 'ave_stability_time','peak_infected','peak_infected_time','ave_peak_infected','ave_peak_infected_time','sim_num'])\n",
    "total_nodes = len(list(arxiv_G.nodes()))\n",
    "\n",
    "for sim_num in range(1): # independent simulations of graph degradation\n",
    "    g_cop = arxiv_G.copy() # creating copy of graph\n",
    "\n",
    "    remove_per_it = np.floor((total_nodes-0.05*total_nodes)/35) # number of nodes to remove in 35 batches to have 5% remaining at the end\n",
    "\n",
    "    config = mc.Configuration() # model parameters\n",
    "    config.add_model_parameter('gamma', 0.01)\n",
    "    config.add_model_parameter(\"fraction_infected\", 0.05)\n",
    "    config.add_model_parameter('beta', 0.015)\n",
    "\n",
    "    model = ep.SIRModel(g_cop)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    trends = multi_runs(model, execution_number=10, iteration_number=1750, infection_sets=None, nprocesses=5) # running the model\n",
    "\n",
    "    num_infected = [[(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]]]\n",
    "    stability_time = [[trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)]]\n",
    "\n",
    "    peak_infected =[[max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]]]\n",
    "    peak_infected_time = [[trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)]]\n",
    "\n",
    "    nodes = [len(list(g_cop.nodes()))]\n",
    "    for i in range(35): # removing nodes in 35 batches\n",
    "        all_nodes = list(g_cop.nodes())\n",
    "        choices = np.random.choice(all_nodes, int(remove_per_it), replace=False)\n",
    "        g_cop.remove_nodes_from(choices) # removing nodes at random\n",
    "\n",
    "        model = ep.SIRModel(g_cop)\n",
    "        model.set_initial_status(config)\n",
    "\n",
    "        trends = multi_runs(model, execution_number=10, iteration_number=1750, infection_sets=None, nprocesses=5) # running the model after removing the nodes\n",
    "\n",
    "        num_infected.append([(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]])\n",
    "        stability_time.append([trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)])\n",
    "\n",
    "        peak_infected.append([max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]])\n",
    "        peak_infected_time.append([trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)])\n",
    "\n",
    "        nodes.append(len(list(g_cop.nodes())))\n",
    "    df = pd.DataFrame({'nodes': nodes, 'num_infected':num_infected, 'stability_time':stability_time, 'peak_infected': peak_infected, 'peak_infected_time': peak_infected_time})\n",
    "    df['ave_num_infected'] = df['num_infected'].apply(lambda col: np.array(col).mean())\n",
    "    df['ave_stability_time'] = df['stability_time'].apply(lambda col: np.array(col).mean())\n",
    "    df['ave_peak_infected'] = df['peak_infected'].apply(lambda col: np.array(col).mean())\n",
    "    df['ave_peak_infected_time'] = df['peak_infected_time'].apply(lambda col: np.array(col).mean())\n",
    "\n",
    "    df['sim_num'] = sim_num\n",
    "\n",
    "    all_results_df = pd.concat([all_results_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SlashDot.com network processing, as in Wang et al. (2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://snap.stanford.edu/data/soc-sign-Slashdot081106.html\n",
    "# Need to unzip the file to a txt and delete meta information first\n",
    "\n",
    "txt_file = pd.read_csv('soc-sign-Slashdot081106.txt', delimiter = '\\t')\n",
    "txt_file = txt_file[['0','1']].loc[txt_file['1.1']!=-1] # removing all negative edges \n",
    "txt_file.to_csv('soc_updated.txt', index=False, sep = '\\t')\n",
    "\n",
    "soc_G = nx.read_edgelist(\"soc_updated.txt\")\n",
    "soc_G = soc_G.to_undirected()\n",
    "soc_G = nx.convert_node_labels_to_integers(soc_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014279708222539718"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*len(soc_G.edges())/len(list(soc_G.nodes()))/len(list(soc_G.nodes()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Node Removal, SlashDot.com network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = pd.DataFrame(columns=['num_infected', 'ave_num_infected', 'stability_time', 'ave_stability_time','peak_infected','peak_infected_time','ave_peak_infected','ave_peak_infected_time','sim_num'])\n",
    "total_nodes = len(list(soc_G.nodes()))\n",
    "\n",
    "for sim_num in range(1): # independent simulations of graph degradation\n",
    "    g_cop = soc_G.copy() # creating copy of graph\n",
    "\n",
    "    remove_per_it = np.floor((total_nodes-0.05*total_nodes)/35) # number of nodes to remove in 35 batches to have 5% remaining at the end\n",
    "\n",
    "    config = mc.Configuration() # model parameters\n",
    "    config.add_model_parameter('gamma', 0.01)\n",
    "    config.add_model_parameter(\"fraction_infected\", 0.05)\n",
    "    config.add_model_parameter('beta', 0.085)\n",
    "\n",
    "    model = ep.SIRModel(g_cop)\n",
    "    model.set_initial_status(config)\n",
    "\n",
    "    trends = multi_runs(model, execution_number=10, iteration_number=1750, infection_sets=None, nprocesses=5) # running the model\n",
    "\n",
    "    num_infected = [[(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]]]\n",
    "    stability_time = [[trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)]]\n",
    "\n",
    "    peak_infected =[[max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]]]\n",
    "    peak_infected_time = [[trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)]]\n",
    "\n",
    "    nodes = [len(list(g_cop.nodes()))]\n",
    "    for i in range(35): # removing nodes in 35 batches\n",
    "        all_nodes = list(g_cop.nodes())\n",
    "        choices = np.random.choice(all_nodes, int(remove_per_it), replace=False)\n",
    "        g_cop.remove_nodes_from(choices) # removing nodes at random\n",
    "\n",
    "        model = ep.SIRModel(g_cop)\n",
    "        model.set_initial_status(config)\n",
    "\n",
    "        trends = multi_runs(model, execution_number=10, iteration_number=1750, infection_sets=None, nprocesses=5) # running the model after removing the nodes\n",
    "\n",
    "        num_infected.append([(len(list(g_cop.nodes())) - x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][0][-1] for j in range(10)]])\n",
    "        stability_time.append([trends[j]['trends']['node_count'][0].index(trends[j]['trends']['node_count'][0][-1]) for j in range(10)])\n",
    "\n",
    "        peak_infected.append([max(x)/len(list(g_cop.nodes())) for x in [trends[j]['trends']['node_count'][1] for j in range(10)]])\n",
    "        peak_infected_time.append([trends[j]['trends']['node_count'][1].index(max(trends[j]['trends']['node_count'][1])) for j in range(10)])\n",
    "\n",
    "        nodes.append(len(list(g_cop.nodes())))\n",
    "    df = pd.DataFrame({'nodes': nodes, 'num_infected':num_infected, 'stability_time':stability_time, 'peak_infected': peak_infected, 'peak_infected_time': peak_infected_time})\n",
    "    df['ave_num_infected'] = df['num_infected'].apply(lambda col: np.array(col).mean())\n",
    "    df['ave_stability_time'] = df['stability_time'].apply(lambda col: np.array(col).mean())\n",
    "    df['ave_peak_infected'] = df['peak_infected'].apply(lambda col: np.array(col).mean())\n",
    "    df['ave_peak_infected_time'] = df['peak_infected_time'].apply(lambda col: np.array(col).mean())\n",
    "\n",
    "    df['sim_num'] = sim_num\n",
    "\n",
    "    all_results_df = pd.concat([all_results_df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network_diffusion_env",
   "language": "python",
   "name": "network_diffusion_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
